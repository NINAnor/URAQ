---
title: "Urban Air Quality"
subtitle: "|
  [NO_URAQ_000 to NO_URAQ_004] - Urban Air Quality - Particulate Matter 2.5 concentration in cties\
  [NO_URGR_100 to NO_URAQ_103] - Urban Air QUality - Ozone concentration in cities"
format: 
  html:
    embed-resources: true
    code-fold: true
    toc: true
    toc-title: Contents
    toc-depth: 3
    smooth-scroll: true
execute: 
  cache: true
author:
  - name: Sylvie Clappe # Enter name
    email: sylvie.clappe@nina.no  # Enter email
    affiliations:
      - id: myID
        name: Norwegian Institute for Nature Research # Enter affiliations
  - name: Bálint Czúcz            #  Enter subsequent authors like this, or remove if not relevant
    email: balint.czucz@nina.no
    affiliations:
      - ref: myID               # To reuse affiliations referecen the id like this
date: July 10, 2025# Enter date 
callout-icon: false
lightbox: true
css: ../../../style.css
code-links:
      - text: Add a review
        icon: github
        href: https://github.com/NINAnor/ecRxiv
---

<!--# This is a template for how to document the indicator analyses. Make sure also to not change the order, or modify, the headers, unless you really need to. This is because it easier to read if all the indicators are presented using the same layout. If there is one header where you don't have anything to write, just leave the header as is, and don't write anything below it. If you are providing code, be careful to annotate and comment on every step in the analysis. Before starting it is recommended to fill in as much as you can in the metadata file. This file will populate the initial table in your output.-->

<!--# Load all you dependencies here -->

```{r setup}
#| include: false
library(knitr)
library(tidyverse)
library(kableExtra)
library(terra)
library(sf)
library(dplyr)
library(readxl)
library(httr2)
library(jsonlite)

library(purrr)
library(exactextractr)

knitr::opts_chunk$set(echo = TRUE)
```

```{r source}
#| echo: false
source(here::here("_common.R"))
```

```{r}
#| echo: false
meta <- readxl::read_xlsx("../metadata.xlsx")
st <- meta |>
  filter(Variable == "status") |>
  pull(Value)
version <- meta |>
  filter(Variable == "Version") |>
  pull(Value)
auth <- meta |>
  filter(Variable == "authors") |>
  pull(Value)
year <- meta |>
  filter(Variable == "yearAdded") |>
  pull(Value)
id <- meta |>
  filter(Variable == "indicatorID") |>
  pull(Value)
name <- meta |>
  filter(Variable == "indicatorName") |>
  pull(Value)
url <- meta |>
  filter(Variable == "url") |>
  pull(Value)

meta <- meta |>
  mutate(Variable = case_match(Variable,
    "indicatorID" ~ "Indicator ID" ,
    "indicatorName" ~ "Indicator Name",
    "country" ~ "Country",
    "continent" ~ "Continent",
    "ECT" ~ "Ecosystem Condition Typology Class",
    "yearAdded" ~ "Year added",
    "yearLastUpdate" ~ "Last update",
    .default = Variable
   )
  ) |>
  filter(Variable != "authors")

```

<!--# The following parts are autogenerated. Do not edit. -->

```{r}
#| echo: false
#| results: asis
status(st)
```

::: {layout-ncol="2"}



> **Recomended citation**: `r paste(auth, " ", year, ". ", name, " (ID: ", id, ") ", "v. ", version, ". ecRxiv: https://view.nina.no/ecRxiv/", sep="")`

> **Version**: `r version`

:::

```{=html}
<details>
<summary>Show metadata</summary>
```

```{r tbl-meta}
#| tbl-cap: 'Indicator metadata'
#| echo: false
#| warning: false

meta |>
  select(Variable, Value) |>
  kbl(col.names = NULL) 

```

```{=html}
</details>
```

::: {.callout-tip collapse="true"}

## Logg

<!--# Update this logg with short messages for each update -->
- 01 Jan. 1901 - Original PR
:::


<hr />

<!--# Document you work below.  -->

## 1. Summary
***What is the urban quality indicators?*** \
Urban air quality can be defined as the degree to which the air in urban areas (*e.g.,* cities, towns, suburbs) is free from pollutants. 

***How to measure air quality in an urban area?*** \
Air quality can be measured by the presence of various pollutants in the air. In this report, we focused on the concentration of two pollutants: particulate matter 2.5 and ozone. There are 3 major steps in calculating this indicator:

1. Identify the total area of urban areas.
2. Within this area, calculate the average yearly concentration of particulate matter 2.5 and ozone.

***How to use and interpret this indicator?*** \
There are several ways of defining urban areas, and different data sources can be used. That is why, in this document, we present five variations of the indicator based on three different analytical approaches. We recommend to use URGR_030 and URGR_031 as they rely on the most ecologically correct definition. They however do not follow the official guidance from the European Commission. If users need to report this indicator in an official EU reporting, we recommend to use URGR_013 or URGR_022.

<!--# 

With a maximum of 300 words, describe the indicator in general terms as you would to a non-expert. Think of this as a kind of commmon language summary. It is a good idea to include a bullet point list of the spesific steps in the workflow. Include a mention of the following aspects:

What does the metric represent?
Why is this relevant for describing ecosystem condition in this ecosystem?
What are the main anthropogenig impact factors?
What kind of data is used? 
Shortly, how is the data customized (modified, estimated, integarted) to fit its purpuse as an indicator?
What is the current status of  the metric (can it be used or is it still in development)?
How should the metric be used and interpretted, and how should it not be used/interpretted?

 -->

## 2. About the underlying data

<!--# Describe the data you have used in more detail, it's origin, biases, availabilit ect.-->

### 2.1 Spatial and temporal resolution and extent

<!--# Describe the temporal and spatial resolution and extent of the data used -->

### 2.2 Original units

<!--# What are the original units for the most relevant  variables in the data-->

### 2.3 Additional comments about the dataset

<!--# Text here -->

#### 2.3.1 Instructions for citing, using and accessing data

<!--# Is the data openly available? If not, how can one access it? What are the key references to the datasets?   -->


## 3. Indicator properties

### 3.1 Ecosystem Condition Typology Class (ECT)

<!--# 

Describe the rationale for assigning the indicator to the ECT class. See https://oneecosystem.pensoft.net/article/58218/
This doesnt need to be very long. Maybe just a single sentence. 

-->

### 3.2 Ecosystem condition characteristic

<!--# 

Describe the ecosystem condition characteristic represented in the indicator. See 10.3897/oneeco.6.e58218 for information on what these characteristics might be.
For example, and indicator called 'Trenching in mires' could be made to represent an ecosystem characteristic 'Intact hydrology'. The term 'characteristic' is used similar to the term 'criteria' in Multiple Criteria Decition Making.  

-->

### 3.3 Other standards

<!--# Optional: Add text about other spesific standards, e.g. national standards, and how the indicator relates to these -->

### 3.4 Collinearities with other indicators

<!--# Describe known collinearities with other metrices (indicators or variables) that could become problematic if they were also included in the same Ecosystem Condition Assessment as the indicator described here. -->


### 3.5 Impact factors

<!--# Describe the main natural and anthropogenic factors that affecst the metric -->


## 4. Reference condition and levels

### 4.1 Reference condition

<!--# Define the reference condition (or refer to where it is defined). Note the destinction between reference condition and reference levels 10.3897/oneeco.5.e58216  -->

### 4.2 Reference levels

<!--# 

If relevant (i.e. if you have normalised a variable), describe the reference levels used to normalise the variable. 

Use the terminology where X~0~ referes to the referece level (the variable value) denoting the worst possible condition; X~100~denotes the optimum or best possible condition; and X~*n*~, where in is between 0 and 100, denotes any other anchoring points linking the variable scale to the indicator scale (e.g. the threshold value between good and bad condition X~60^). 

Why was the current option chosen and how were the reference levels quantified? If the reference values are calculated as part of the analyses further down, please repeat the main information here.

 -->


#### 4.2.1 Spatial resolution and validity

<!--# 

Describe the spatial resolution of the reference levels. E.g. is it defined as a fixed value for all areas, or does it vary. Also, at what spatial scale are the reference levels valid? For example, if the reference levels have a regional resolution (varies between regions), it might mean that it is only valid and correct to use for normalising local variable values that are first aggregated to regional scale. However, sometimes the reference levels are insensitive to this and can be used to scale variables at the local (e.g. plot) scale. 

 -->

## 5. Uncertainties

<!--# Describe the main uncertainties or sources of error in the indicator or the underlying data. -->

## 6. References

<!--# You can add references manually or use a citation manager and add intext citations as with crossreferencing and hyperlinks. See https://quarto.org/docs/authoring/footnotes-and-citations.html -->

## 7. Datasets

<!--# Describe the unique datasets seperately under seperate headers (Dataset A, Dataset B, etc.-->

### 7.1 Dataset A

<!--# Describe the main dataset, typicaly the one containing the variable of (most) interest. Change the header from Dataset A to the name of the actuall dataset. -->

### 7.2. Dataset B

<!--# Describe additional datasets in a similar was as above. Deleteor add ned subheaders as needed. -->

## 8. Spatial units

<!--# 

Describe the spatial units that you rely on in your analyses. Highlight the spatial units (the resolution) that the indicator values should be interpretted at. Potential spatial delineation data should eb introduced under 7.1. Datasets. We recomend using the SEEA EA terminology opf Basic Spatial Units (BSU), Ecosystem Asses (EA) and Ecosystem Accounting Area (EAA). 

-->

## 9. Analyses

### Paths to data sources
Before starting any of the analyses, we store the path in which the data will be found. If readers wish to download the data and run this code, they can just change the paths below to their local paths for the rest of the code to function.
```{r}
#| eval: false
#| code-summary: "Paths to the data"

# DEGURBA from Eurostat
path_degurba <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/LAU_DEGURBA_Eurostat/EU-27-LAU-2024-NUTS-2024.xlsx"

# LAUs shapefile from Eurostat
path_lau <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/LAU_Eurostat/LAU_RG_01M_2023_3035.shp"

# Municipalities from Kartverkert
path_kom <- "/data/R/GeoSpatialData/AdministrativeUnits/Norway_AdministrativeUnits/Original/Norway_Municipalities/Administrative enheter kommuner FGDB-format/Basisdata_0000_Norge_25833_Kommuner_FGDB/Basisdata_0000_Norge_25833_Kommuner_FGDB.gdb"

# Municipalities number 2023 and 2024
path_kom_nb <- "/data/P-Prosjekter2/412413_2023_no_egd/git_data/Adminstrative_boundaries_NO/NO_Fylke/kommune_fylke_region_NO_2024.xlsx"
```

### Define urban areas
In all the versions of the URAQ indicators defined in this document, the urban area is either defined as (i) the total area of Local Administrative Units (LAUs); (ii) the total area of EU Typology class "Settlements and Other Artificial Areas" (SOAA) within the LAUs; or (iii) the total area of SOAA in Norway. We need to create the relevant datasets for the compilation of areas (i) and (ii). Area (iii) is the total of all the areas (ii).

#### Local Administrative Units (LAUs)
The Local Administrative Units in Norway are the municipalities. A shapefile with the administrative boundaries of the municipalities is available from Eurostat. As we are not interested in the marine ecosystems, we chose to use Eurotat boundaries instead of those available nationally from Kartverkert as the boundaries excluded the sea. Unfortunately, Eurostat shapefile is from 2023, before the modification of municipalities that happened in 2024, we thus have to manipulate the shapefile from Eurostat to update it in for the municipalities Haram and Ålesund.
```{r}
#| eval: false
#| code-summary: "Update Eurotate LAUs to 2024"

# Read shapefile and select Norway
lau_no <- st_read(path_lau) %>%
           filter(CNTR_CODE == "NO")

# Read the shapefile from Kartverkert
kom_2024 <- st_read(path_kom, layer = "kommune")

# Select Haram and Ålesund municipalities
har_al_lau <- lau_no %>% 
                filter(LAU_ID == 1507)

har_al_kom <- kom_2024 %>%
                filter(kommunenummer == 1508 | kommunenummer == 1580) %>%
                st_transform(., st_crs(har_al_lau))

# Intersection between Eurostat LAU shapefile and Kartverkert 
# municipality shapefile for Haram and Ålesund municipalities 
kom_nb <- read_excel(path_kom_nb)

haram_2024 <- st_intersection(har_al_lau, har_al_kom[2,]) %>%
                select(colnames(lau_no)) %>%
                left_join(., kom_nb, by = join_by("LAU_ID" == "kommune_nr_2023")) %>%
                select(LAU_NAME, fylke_nr, kommune_nr_2024, region_name, geometry)

alesund_2024 <- st_intersection(har_al_lau, har_al_kom[1,]) %>%
                select(colnames(lau_no)) %>%
                left_join(., kom_nb, by = join_by("LAU_ID" == "kommune_nr_2023")) %>%
                select(LAU_NAME, fylke_nr, kommune_nr_2024, region_name, geometry)

# Final LAUs shapefile
lau_no_2024 <- lau_no %>%
                filter(!LAU_ID == 1507) %>%
                left_join(., kom_nb, by = join_by("LAU_ID" == "kommune_nr_2023")) %>%
                select(LAU_NAME, fylke_nr, kommune_nr_2024, region_name, geometry) %>%
                rbind(., haram_2024, alesund_2024) %>%
                st_as_sf()
                
                
```

To compile the different versions of the URAQ indicators, we need to append to this shapefile the variable `DEGURBA`, which represent the degree of urbanisation of the municipality. This variable is produced at European level by Eurostat and is freely available. In the code below we join the `DEGURBA` variable to the municipality shapefile. 
```{r}
#| eval: false
#| code-summary: "Add the DEGURBA variable to the LAU dataset"

# Read excel file with Degurba variable and select Norway
degurba_lau_no <- read_excel(path_degurba, sheet = "NO")

degurba_lau_no$`LAU CODE` <- as.numeric(degurba_lau_no$`LAU CODE`)
degurba_lau_no$`LAU CODE`[which(degurba_lau_no$`LAU CODE` == 0301)] <- 301

# Join lau_no_2024 with DEGURBA
lau_dgurb <- inner_join(lau_no_2024,
                        degurba_lau_no,
                       by = join_by("kommune_nr_2024" == "LAU CODE")) %>%
              select(LAU_NAME, DEGURBA, fylke_nr, kommune_nr_2024, region_name, geometry) %>%
              st_as_sf()

```

We can now read in the PM2.5 concentration maps for each of the municipalities. As the PM2.5 maps are available from Miljødirektoratet website using an API, we first create a list of the maps that should be accessed before retrieving the data through the API.
```{r}
#| eval: false
#| code-summary: "Load Pm2.5 concentration maps"




```







### PM2.5 concentration


### Summary table to derive URAQ
Now we can build an overall table that we will be able to use to calculate all the versions of the URAQ indicator in the Results section below.






### TEXT FOR OTHER INDICATORS
Now, we can load the particulate matter 2.5 (PM2.5) data directly using an API. This data are individual maps for all municipalities so we need to create a geodatabase to regroup all of them. The reason we create a geodatabase instead of just working with the data directly obtained through the API is that we will perform spatial intersections with the `duckdb` package. Our spatial intersections are computing intensive and using the `duckdb` package allows to significantly decrease the computing time. `duckdb` however works directly with the databases through SQL commands, hence the need to create our geodatabase first.
```{r}
#| eval: false
#| code-summary: "Create PM2.5 geodatabase"


```










<!--# 

Use this header for documenting the analyses. Put code in seperate code chunks, and annotate the code in between using normal text (i.e. between the chunks, and try to avoid too many hashed out comments inside the code chunks). Add subheaders as needed. 

Code folding is activated, meaning the code will be hidden by default in the html (one can click to expand it).

Caching is also activated (from the top YAML), meaning that rendering to html will be quicker the second time you do it. This will create a folder inside you project folder (called INDICATORID_cache). Sometimes caching created problems because some operations are not rerun when they should be rerun. Try deleting the cash folder and try again.

-->

## 10. Results

<!--# 

Repeat the final results here. Typically this is a map or table of indicator values.

This is typically where people will harvest data from, so make sure to include all relevant output here, but don't clutter this section with too much output either.

-->

## 11. Export file

<!--# 

Optional: Display the code (don't execute it) or the workflow for exporting the indicator values to file. Ideally the indicator values are exported as a georeferenced shape or raster file with indicators values, reference values and errors. You can also chose to export the raw (un-normalised or unscaled variable) as a seperate product. You should not save large sptaial output data on GitHub. You can use eval=FALSE to avoid code from being executed (example below - delete if not relevant) 

-->

```{r export}
#| eval: false
```
